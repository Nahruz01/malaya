{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642d73c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-28 22:04:14,830] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from transformers import AddedToken, AutoModel, AutoTokenizer, AutoProcessor, Qwen2ForCausalLM, AutoConfig\n",
    "from transformers.models.whisper.modeling_whisper import WhisperEncoder\n",
    "from datasets import Audio\n",
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e0f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Qwen2ForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder = WhisperEncoder(config.audio_encoder_config)\n",
    "        self.projection = nn.Linear(self.encoder.config.d_model, self.config.hidden_size, bias=True)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask, \n",
    "        input_features = None, \n",
    "        feature_attention_mask = None, \n",
    "        labels = None, \n",
    "        **kwargs,\n",
    "    ):\n",
    "        inputs_embeds = self.get_input_embeddings()(input_ids)\n",
    "        if input_features is not None:\n",
    "            batch_size, _, max_mel_seq_len = input_features.shape\n",
    "            max_seq_len = (max_mel_seq_len - 2) // 2 + 1\n",
    "            audio_feat_lengths = self.encoder._get_feat_extract_output_lengths(feature_attention_mask.sum(-1))\n",
    "            seq_range = (\n",
    "                torch.arange(0, max_seq_len, dtype=audio_feat_lengths.dtype, device=audio_feat_lengths.device)\n",
    "                .unsqueeze(0)\n",
    "                .expand(batch_size, max_seq_len)\n",
    "            )\n",
    "            lengths_expand = audio_feat_lengths.unsqueeze(1).expand(batch_size, max_seq_len)\n",
    "            padding_mask = seq_range >= lengths_expand\n",
    "\n",
    "            audio_attention_mask_ = padding_mask.view(batch_size, 1, 1, max_seq_len).expand(\n",
    "                batch_size, 1, max_seq_len, max_seq_len\n",
    "            )\n",
    "            audio_attention_mask = audio_attention_mask_.to(\n",
    "                dtype=self.encoder.conv1.weight.dtype, device=self.encoder.conv1.weight.device\n",
    "            )\n",
    "            audio_attention_mask[audio_attention_mask_] = float(\"-inf\")\n",
    "            audio_outputs = self.encoder(input_features, attention_mask=audio_attention_mask)\n",
    "            selected_audio_feature = audio_outputs.last_hidden_state\n",
    "            audio_features = self.projection(selected_audio_feature)\n",
    "            num_audio_tokens = audio_feat_lengths\n",
    "            num_audios, max_audio_tokens, embed_dim = audio_features.shape\n",
    "            audio_features_mask = torch.arange(max_audio_tokens).expand(num_audios, max_audio_tokens).to(\n",
    "                num_audio_tokens.device\n",
    "            ) < num_audio_tokens.unsqueeze(1)\n",
    "            masked_audio_features = audio_features[audio_features_mask].view(-1, embed_dim)\n",
    "            inputs_embeds[input_ids == model.config.audio_token_index] = masked_audio_features.contiguous()\n",
    "        \n",
    "        super_out = self.model.forward(\n",
    "            inputs_embeds = inputs_embeds, \n",
    "            attention_mask = attention_mask,\n",
    "            output_hidden_states = True,\n",
    "        )\n",
    "        return super_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5efc8573",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-7B-Instruct')\n",
    "processor = AutoProcessor.from_pretrained('openai/whisper-large-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b65047",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = \"{% set audio_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if 'audio' in content or 'audio_url' in content or message['type'] == 'audio' %}{% set audio_count.value = audio_count.value + 1 %}Audio {{ audio_count.value }}: <|audio_bos|><|file_sep|><|audio_eos|>\\n{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7866415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained('Qwen/Qwen2.5-7B-Instruct')\n",
    "audio_encoder_config = AutoConfig.from_pretrained('huseinzol05/whisper-large-v3-encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be428028",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.audio_encoder_config = audio_encoder_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dfed9f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe58640073c4521bf290a04e3ac3ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Model were not initialized from the model checkpoint at Qwen/Qwen2.5-7B-Instruct and are newly initialized: ['encoder.conv1.bias', 'encoder.conv1.weight', 'encoder.conv2.bias', 'encoder.conv2.weight', 'encoder.embed_positions.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.fc1.bias', 'encoder.layers.0.fc1.weight', 'encoder.layers.0.fc2.bias', 'encoder.layers.0.fc2.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.self_attn.k_proj.weight', 'encoder.layers.0.self_attn.out_proj.bias', 'encoder.layers.0.self_attn.out_proj.weight', 'encoder.layers.0.self_attn.q_proj.bias', 'encoder.layers.0.self_attn.q_proj.weight', 'encoder.layers.0.self_attn.v_proj.bias', 'encoder.layers.0.self_attn.v_proj.weight', 'encoder.layers.0.self_attn_layer_norm.bias', 'encoder.layers.0.self_attn_layer_norm.weight', 'encoder.layers.1.fc1.bias', 'encoder.layers.1.fc1.weight', 'encoder.layers.1.fc2.bias', 'encoder.layers.1.fc2.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.self_attn.k_proj.weight', 'encoder.layers.1.self_attn.out_proj.bias', 'encoder.layers.1.self_attn.out_proj.weight', 'encoder.layers.1.self_attn.q_proj.bias', 'encoder.layers.1.self_attn.q_proj.weight', 'encoder.layers.1.self_attn.v_proj.bias', 'encoder.layers.1.self_attn.v_proj.weight', 'encoder.layers.1.self_attn_layer_norm.bias', 'encoder.layers.1.self_attn_layer_norm.weight', 'encoder.layers.10.fc1.bias', 'encoder.layers.10.fc1.weight', 'encoder.layers.10.fc2.bias', 'encoder.layers.10.fc2.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.self_attn.k_proj.weight', 'encoder.layers.10.self_attn.out_proj.bias', 'encoder.layers.10.self_attn.out_proj.weight', 'encoder.layers.10.self_attn.q_proj.bias', 'encoder.layers.10.self_attn.q_proj.weight', 'encoder.layers.10.self_attn.v_proj.bias', 'encoder.layers.10.self_attn.v_proj.weight', 'encoder.layers.10.self_attn_layer_norm.bias', 'encoder.layers.10.self_attn_layer_norm.weight', 'encoder.layers.11.fc1.bias', 'encoder.layers.11.fc1.weight', 'encoder.layers.11.fc2.bias', 'encoder.layers.11.fc2.weight', 'encoder.layers.11.final_layer_norm.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.self_attn.k_proj.weight', 'encoder.layers.11.self_attn.out_proj.bias', 'encoder.layers.11.self_attn.out_proj.weight', 'encoder.layers.11.self_attn.q_proj.bias', 'encoder.layers.11.self_attn.q_proj.weight', 'encoder.layers.11.self_attn.v_proj.bias', 'encoder.layers.11.self_attn.v_proj.weight', 'encoder.layers.11.self_attn_layer_norm.bias', 'encoder.layers.11.self_attn_layer_norm.weight', 'encoder.layers.12.fc1.bias', 'encoder.layers.12.fc1.weight', 'encoder.layers.12.fc2.bias', 'encoder.layers.12.fc2.weight', 'encoder.layers.12.final_layer_norm.bias', 'encoder.layers.12.final_layer_norm.weight', 'encoder.layers.12.self_attn.k_proj.weight', 'encoder.layers.12.self_attn.out_proj.bias', 'encoder.layers.12.self_attn.out_proj.weight', 'encoder.layers.12.self_attn.q_proj.bias', 'encoder.layers.12.self_attn.q_proj.weight', 'encoder.layers.12.self_attn.v_proj.bias', 'encoder.layers.12.self_attn.v_proj.weight', 'encoder.layers.12.self_attn_layer_norm.bias', 'encoder.layers.12.self_attn_layer_norm.weight', 'encoder.layers.13.fc1.bias', 'encoder.layers.13.fc1.weight', 'encoder.layers.13.fc2.bias', 'encoder.layers.13.fc2.weight', 'encoder.layers.13.final_layer_norm.bias', 'encoder.layers.13.final_layer_norm.weight', 'encoder.layers.13.self_attn.k_proj.weight', 'encoder.layers.13.self_attn.out_proj.bias', 'encoder.layers.13.self_attn.out_proj.weight', 'encoder.layers.13.self_attn.q_proj.bias', 'encoder.layers.13.self_attn.q_proj.weight', 'encoder.layers.13.self_attn.v_proj.bias', 'encoder.layers.13.self_attn.v_proj.weight', 'encoder.layers.13.self_attn_layer_norm.bias', 'encoder.layers.13.self_attn_layer_norm.weight', 'encoder.layers.14.fc1.bias', 'encoder.layers.14.fc1.weight', 'encoder.layers.14.fc2.bias', 'encoder.layers.14.fc2.weight', 'encoder.layers.14.final_layer_norm.bias', 'encoder.layers.14.final_layer_norm.weight', 'encoder.layers.14.self_attn.k_proj.weight', 'encoder.layers.14.self_attn.out_proj.bias', 'encoder.layers.14.self_attn.out_proj.weight', 'encoder.layers.14.self_attn.q_proj.bias', 'encoder.layers.14.self_attn.q_proj.weight', 'encoder.layers.14.self_attn.v_proj.bias', 'encoder.layers.14.self_attn.v_proj.weight', 'encoder.layers.14.self_attn_layer_norm.bias', 'encoder.layers.14.self_attn_layer_norm.weight', 'encoder.layers.15.fc1.bias', 'encoder.layers.15.fc1.weight', 'encoder.layers.15.fc2.bias', 'encoder.layers.15.fc2.weight', 'encoder.layers.15.final_layer_norm.bias', 'encoder.layers.15.final_layer_norm.weight', 'encoder.layers.15.self_attn.k_proj.weight', 'encoder.layers.15.self_attn.out_proj.bias', 'encoder.layers.15.self_attn.out_proj.weight', 'encoder.layers.15.self_attn.q_proj.bias', 'encoder.layers.15.self_attn.q_proj.weight', 'encoder.layers.15.self_attn.v_proj.bias', 'encoder.layers.15.self_attn.v_proj.weight', 'encoder.layers.15.self_attn_layer_norm.bias', 'encoder.layers.15.self_attn_layer_norm.weight', 'encoder.layers.16.fc1.bias', 'encoder.layers.16.fc1.weight', 'encoder.layers.16.fc2.bias', 'encoder.layers.16.fc2.weight', 'encoder.layers.16.final_layer_norm.bias', 'encoder.layers.16.final_layer_norm.weight', 'encoder.layers.16.self_attn.k_proj.weight', 'encoder.layers.16.self_attn.out_proj.bias', 'encoder.layers.16.self_attn.out_proj.weight', 'encoder.layers.16.self_attn.q_proj.bias', 'encoder.layers.16.self_attn.q_proj.weight', 'encoder.layers.16.self_attn.v_proj.bias', 'encoder.layers.16.self_attn.v_proj.weight', 'encoder.layers.16.self_attn_layer_norm.bias', 'encoder.layers.16.self_attn_layer_norm.weight', 'encoder.layers.17.fc1.bias', 'encoder.layers.17.fc1.weight', 'encoder.layers.17.fc2.bias', 'encoder.layers.17.fc2.weight', 'encoder.layers.17.final_layer_norm.bias', 'encoder.layers.17.final_layer_norm.weight', 'encoder.layers.17.self_attn.k_proj.weight', 'encoder.layers.17.self_attn.out_proj.bias', 'encoder.layers.17.self_attn.out_proj.weight', 'encoder.layers.17.self_attn.q_proj.bias', 'encoder.layers.17.self_attn.q_proj.weight', 'encoder.layers.17.self_attn.v_proj.bias', 'encoder.layers.17.self_attn.v_proj.weight', 'encoder.layers.17.self_attn_layer_norm.bias', 'encoder.layers.17.self_attn_layer_norm.weight', 'encoder.layers.18.fc1.bias', 'encoder.layers.18.fc1.weight', 'encoder.layers.18.fc2.bias', 'encoder.layers.18.fc2.weight', 'encoder.layers.18.final_layer_norm.bias', 'encoder.layers.18.final_layer_norm.weight', 'encoder.layers.18.self_attn.k_proj.weight', 'encoder.layers.18.self_attn.out_proj.bias', 'encoder.layers.18.self_attn.out_proj.weight', 'encoder.layers.18.self_attn.q_proj.bias', 'encoder.layers.18.self_attn.q_proj.weight', 'encoder.layers.18.self_attn.v_proj.bias', 'encoder.layers.18.self_attn.v_proj.weight', 'encoder.layers.18.self_attn_layer_norm.bias', 'encoder.layers.18.self_attn_layer_norm.weight', 'encoder.layers.19.fc1.bias', 'encoder.layers.19.fc1.weight', 'encoder.layers.19.fc2.bias', 'encoder.layers.19.fc2.weight', 'encoder.layers.19.final_layer_norm.bias', 'encoder.layers.19.final_layer_norm.weight', 'encoder.layers.19.self_attn.k_proj.weight', 'encoder.layers.19.self_attn.out_proj.bias', 'encoder.layers.19.self_attn.out_proj.weight', 'encoder.layers.19.self_attn.q_proj.bias', 'encoder.layers.19.self_attn.q_proj.weight', 'encoder.layers.19.self_attn.v_proj.bias', 'encoder.layers.19.self_attn.v_proj.weight', 'encoder.layers.19.self_attn_layer_norm.bias', 'encoder.layers.19.self_attn_layer_norm.weight', 'encoder.layers.2.fc1.bias', 'encoder.layers.2.fc1.weight', 'encoder.layers.2.fc2.bias', 'encoder.layers.2.fc2.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.self_attn.k_proj.weight', 'encoder.layers.2.self_attn.out_proj.bias', 'encoder.layers.2.self_attn.out_proj.weight', 'encoder.layers.2.self_attn.q_proj.bias', 'encoder.layers.2.self_attn.q_proj.weight', 'encoder.layers.2.self_attn.v_proj.bias', 'encoder.layers.2.self_attn.v_proj.weight', 'encoder.layers.2.self_attn_layer_norm.bias', 'encoder.layers.2.self_attn_layer_norm.weight', 'encoder.layers.20.fc1.bias', 'encoder.layers.20.fc1.weight', 'encoder.layers.20.fc2.bias', 'encoder.layers.20.fc2.weight', 'encoder.layers.20.final_layer_norm.bias', 'encoder.layers.20.final_layer_norm.weight', 'encoder.layers.20.self_attn.k_proj.weight', 'encoder.layers.20.self_attn.out_proj.bias', 'encoder.layers.20.self_attn.out_proj.weight', 'encoder.layers.20.self_attn.q_proj.bias', 'encoder.layers.20.self_attn.q_proj.weight', 'encoder.layers.20.self_attn.v_proj.bias', 'encoder.layers.20.self_attn.v_proj.weight', 'encoder.layers.20.self_attn_layer_norm.bias', 'encoder.layers.20.self_attn_layer_norm.weight', 'encoder.layers.21.fc1.bias', 'encoder.layers.21.fc1.weight', 'encoder.layers.21.fc2.bias', 'encoder.layers.21.fc2.weight', 'encoder.layers.21.final_layer_norm.bias', 'encoder.layers.21.final_layer_norm.weight', 'encoder.layers.21.self_attn.k_proj.weight', 'encoder.layers.21.self_attn.out_proj.bias', 'encoder.layers.21.self_attn.out_proj.weight', 'encoder.layers.21.self_attn.q_proj.bias', 'encoder.layers.21.self_attn.q_proj.weight', 'encoder.layers.21.self_attn.v_proj.bias', 'encoder.layers.21.self_attn.v_proj.weight', 'encoder.layers.21.self_attn_layer_norm.bias', 'encoder.layers.21.self_attn_layer_norm.weight', 'encoder.layers.22.fc1.bias', 'encoder.layers.22.fc1.weight', 'encoder.layers.22.fc2.bias', 'encoder.layers.22.fc2.weight', 'encoder.layers.22.final_layer_norm.bias', 'encoder.layers.22.final_layer_norm.weight', 'encoder.layers.22.self_attn.k_proj.weight', 'encoder.layers.22.self_attn.out_proj.bias', 'encoder.layers.22.self_attn.out_proj.weight', 'encoder.layers.22.self_attn.q_proj.bias', 'encoder.layers.22.self_attn.q_proj.weight', 'encoder.layers.22.self_attn.v_proj.bias', 'encoder.layers.22.self_attn.v_proj.weight', 'encoder.layers.22.self_attn_layer_norm.bias', 'encoder.layers.22.self_attn_layer_norm.weight', 'encoder.layers.23.fc1.bias', 'encoder.layers.23.fc1.weight', 'encoder.layers.23.fc2.bias', 'encoder.layers.23.fc2.weight', 'encoder.layers.23.final_layer_norm.bias', 'encoder.layers.23.final_layer_norm.weight', 'encoder.layers.23.self_attn.k_proj.weight', 'encoder.layers.23.self_attn.out_proj.bias', 'encoder.layers.23.self_attn.out_proj.weight', 'encoder.layers.23.self_attn.q_proj.bias', 'encoder.layers.23.self_attn.q_proj.weight', 'encoder.layers.23.self_attn.v_proj.bias', 'encoder.layers.23.self_attn.v_proj.weight', 'encoder.layers.23.self_attn_layer_norm.bias', 'encoder.layers.23.self_attn_layer_norm.weight', 'encoder.layers.24.fc1.bias', 'encoder.layers.24.fc1.weight', 'encoder.layers.24.fc2.bias', 'encoder.layers.24.fc2.weight', 'encoder.layers.24.final_layer_norm.bias', 'encoder.layers.24.final_layer_norm.weight', 'encoder.layers.24.self_attn.k_proj.weight', 'encoder.layers.24.self_attn.out_proj.bias', 'encoder.layers.24.self_attn.out_proj.weight', 'encoder.layers.24.self_attn.q_proj.bias', 'encoder.layers.24.self_attn.q_proj.weight', 'encoder.layers.24.self_attn.v_proj.bias', 'encoder.layers.24.self_attn.v_proj.weight', 'encoder.layers.24.self_attn_layer_norm.bias', 'encoder.layers.24.self_attn_layer_norm.weight', 'encoder.layers.25.fc1.bias', 'encoder.layers.25.fc1.weight', 'encoder.layers.25.fc2.bias', 'encoder.layers.25.fc2.weight', 'encoder.layers.25.final_layer_norm.bias', 'encoder.layers.25.final_layer_norm.weight', 'encoder.layers.25.self_attn.k_proj.weight', 'encoder.layers.25.self_attn.out_proj.bias', 'encoder.layers.25.self_attn.out_proj.weight', 'encoder.layers.25.self_attn.q_proj.bias', 'encoder.layers.25.self_attn.q_proj.weight', 'encoder.layers.25.self_attn.v_proj.bias', 'encoder.layers.25.self_attn.v_proj.weight', 'encoder.layers.25.self_attn_layer_norm.bias', 'encoder.layers.25.self_attn_layer_norm.weight', 'encoder.layers.26.fc1.bias', 'encoder.layers.26.fc1.weight', 'encoder.layers.26.fc2.bias', 'encoder.layers.26.fc2.weight', 'encoder.layers.26.final_layer_norm.bias', 'encoder.layers.26.final_layer_norm.weight', 'encoder.layers.26.self_attn.k_proj.weight', 'encoder.layers.26.self_attn.out_proj.bias', 'encoder.layers.26.self_attn.out_proj.weight', 'encoder.layers.26.self_attn.q_proj.bias', 'encoder.layers.26.self_attn.q_proj.weight', 'encoder.layers.26.self_attn.v_proj.bias', 'encoder.layers.26.self_attn.v_proj.weight', 'encoder.layers.26.self_attn_layer_norm.bias', 'encoder.layers.26.self_attn_layer_norm.weight', 'encoder.layers.27.fc1.bias', 'encoder.layers.27.fc1.weight', 'encoder.layers.27.fc2.bias', 'encoder.layers.27.fc2.weight', 'encoder.layers.27.final_layer_norm.bias', 'encoder.layers.27.final_layer_norm.weight', 'encoder.layers.27.self_attn.k_proj.weight', 'encoder.layers.27.self_attn.out_proj.bias', 'encoder.layers.27.self_attn.out_proj.weight', 'encoder.layers.27.self_attn.q_proj.bias', 'encoder.layers.27.self_attn.q_proj.weight', 'encoder.layers.27.self_attn.v_proj.bias', 'encoder.layers.27.self_attn.v_proj.weight', 'encoder.layers.27.self_attn_layer_norm.bias', 'encoder.layers.27.self_attn_layer_norm.weight', 'encoder.layers.28.fc1.bias', 'encoder.layers.28.fc1.weight', 'encoder.layers.28.fc2.bias', 'encoder.layers.28.fc2.weight', 'encoder.layers.28.final_layer_norm.bias', 'encoder.layers.28.final_layer_norm.weight', 'encoder.layers.28.self_attn.k_proj.weight', 'encoder.layers.28.self_attn.out_proj.bias', 'encoder.layers.28.self_attn.out_proj.weight', 'encoder.layers.28.self_attn.q_proj.bias', 'encoder.layers.28.self_attn.q_proj.weight', 'encoder.layers.28.self_attn.v_proj.bias', 'encoder.layers.28.self_attn.v_proj.weight', 'encoder.layers.28.self_attn_layer_norm.bias', 'encoder.layers.28.self_attn_layer_norm.weight', 'encoder.layers.29.fc1.bias', 'encoder.layers.29.fc1.weight', 'encoder.layers.29.fc2.bias', 'encoder.layers.29.fc2.weight', 'encoder.layers.29.final_layer_norm.bias', 'encoder.layers.29.final_layer_norm.weight', 'encoder.layers.29.self_attn.k_proj.weight', 'encoder.layers.29.self_attn.out_proj.bias', 'encoder.layers.29.self_attn.out_proj.weight', 'encoder.layers.29.self_attn.q_proj.bias', 'encoder.layers.29.self_attn.q_proj.weight', 'encoder.layers.29.self_attn.v_proj.bias', 'encoder.layers.29.self_attn.v_proj.weight', 'encoder.layers.29.self_attn_layer_norm.bias', 'encoder.layers.29.self_attn_layer_norm.weight', 'encoder.layers.3.fc1.bias', 'encoder.layers.3.fc1.weight', 'encoder.layers.3.fc2.bias', 'encoder.layers.3.fc2.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.self_attn.k_proj.weight', 'encoder.layers.3.self_attn.out_proj.bias', 'encoder.layers.3.self_attn.out_proj.weight', 'encoder.layers.3.self_attn.q_proj.bias', 'encoder.layers.3.self_attn.q_proj.weight', 'encoder.layers.3.self_attn.v_proj.bias', 'encoder.layers.3.self_attn.v_proj.weight', 'encoder.layers.3.self_attn_layer_norm.bias', 'encoder.layers.3.self_attn_layer_norm.weight', 'encoder.layers.30.fc1.bias', 'encoder.layers.30.fc1.weight', 'encoder.layers.30.fc2.bias', 'encoder.layers.30.fc2.weight', 'encoder.layers.30.final_layer_norm.bias', 'encoder.layers.30.final_layer_norm.weight', 'encoder.layers.30.self_attn.k_proj.weight', 'encoder.layers.30.self_attn.out_proj.bias', 'encoder.layers.30.self_attn.out_proj.weight', 'encoder.layers.30.self_attn.q_proj.bias', 'encoder.layers.30.self_attn.q_proj.weight', 'encoder.layers.30.self_attn.v_proj.bias', 'encoder.layers.30.self_attn.v_proj.weight', 'encoder.layers.30.self_attn_layer_norm.bias', 'encoder.layers.30.self_attn_layer_norm.weight', 'encoder.layers.31.fc1.bias', 'encoder.layers.31.fc1.weight', 'encoder.layers.31.fc2.bias', 'encoder.layers.31.fc2.weight', 'encoder.layers.31.final_layer_norm.bias', 'encoder.layers.31.final_layer_norm.weight', 'encoder.layers.31.self_attn.k_proj.weight', 'encoder.layers.31.self_attn.out_proj.bias', 'encoder.layers.31.self_attn.out_proj.weight', 'encoder.layers.31.self_attn.q_proj.bias', 'encoder.layers.31.self_attn.q_proj.weight', 'encoder.layers.31.self_attn.v_proj.bias', 'encoder.layers.31.self_attn.v_proj.weight', 'encoder.layers.31.self_attn_layer_norm.bias', 'encoder.layers.31.self_attn_layer_norm.weight', 'encoder.layers.4.fc1.bias', 'encoder.layers.4.fc1.weight', 'encoder.layers.4.fc2.bias', 'encoder.layers.4.fc2.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.self_attn.k_proj.weight', 'encoder.layers.4.self_attn.out_proj.bias', 'encoder.layers.4.self_attn.out_proj.weight', 'encoder.layers.4.self_attn.q_proj.bias', 'encoder.layers.4.self_attn.q_proj.weight', 'encoder.layers.4.self_attn.v_proj.bias', 'encoder.layers.4.self_attn.v_proj.weight', 'encoder.layers.4.self_attn_layer_norm.bias', 'encoder.layers.4.self_attn_layer_norm.weight', 'encoder.layers.5.fc1.bias', 'encoder.layers.5.fc1.weight', 'encoder.layers.5.fc2.bias', 'encoder.layers.5.fc2.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.self_attn.k_proj.weight', 'encoder.layers.5.self_attn.out_proj.bias', 'encoder.layers.5.self_attn.out_proj.weight', 'encoder.layers.5.self_attn.q_proj.bias', 'encoder.layers.5.self_attn.q_proj.weight', 'encoder.layers.5.self_attn.v_proj.bias', 'encoder.layers.5.self_attn.v_proj.weight', 'encoder.layers.5.self_attn_layer_norm.bias', 'encoder.layers.5.self_attn_layer_norm.weight', 'encoder.layers.6.fc1.bias', 'encoder.layers.6.fc1.weight', 'encoder.layers.6.fc2.bias', 'encoder.layers.6.fc2.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.self_attn.k_proj.weight', 'encoder.layers.6.self_attn.out_proj.bias', 'encoder.layers.6.self_attn.out_proj.weight', 'encoder.layers.6.self_attn.q_proj.bias', 'encoder.layers.6.self_attn.q_proj.weight', 'encoder.layers.6.self_attn.v_proj.bias', 'encoder.layers.6.self_attn.v_proj.weight', 'encoder.layers.6.self_attn_layer_norm.bias', 'encoder.layers.6.self_attn_layer_norm.weight', 'encoder.layers.7.fc1.bias', 'encoder.layers.7.fc1.weight', 'encoder.layers.7.fc2.bias', 'encoder.layers.7.fc2.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.self_attn.k_proj.weight', 'encoder.layers.7.self_attn.out_proj.bias', 'encoder.layers.7.self_attn.out_proj.weight', 'encoder.layers.7.self_attn.q_proj.bias', 'encoder.layers.7.self_attn.q_proj.weight', 'encoder.layers.7.self_attn.v_proj.bias', 'encoder.layers.7.self_attn.v_proj.weight', 'encoder.layers.7.self_attn_layer_norm.bias', 'encoder.layers.7.self_attn_layer_norm.weight', 'encoder.layers.8.fc1.bias', 'encoder.layers.8.fc1.weight', 'encoder.layers.8.fc2.bias', 'encoder.layers.8.fc2.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.self_attn.k_proj.weight', 'encoder.layers.8.self_attn.out_proj.bias', 'encoder.layers.8.self_attn.out_proj.weight', 'encoder.layers.8.self_attn.q_proj.bias', 'encoder.layers.8.self_attn.q_proj.weight', 'encoder.layers.8.self_attn.v_proj.bias', 'encoder.layers.8.self_attn.v_proj.weight', 'encoder.layers.8.self_attn_layer_norm.bias', 'encoder.layers.8.self_attn_layer_norm.weight', 'encoder.layers.9.fc1.bias', 'encoder.layers.9.fc1.weight', 'encoder.layers.9.fc2.bias', 'encoder.layers.9.fc2.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.self_attn.k_proj.weight', 'encoder.layers.9.self_attn.out_proj.bias', 'encoder.layers.9.self_attn.out_proj.weight', 'encoder.layers.9.self_attn.q_proj.bias', 'encoder.layers.9.self_attn.q_proj.weight', 'encoder.layers.9.self_attn.v_proj.bias', 'encoder.layers.9.self_attn.v_proj.weight', 'encoder.layers.9.self_attn_layer_norm.bias', 'encoder.layers.9.self_attn_layer_norm.weight', 'projection.bias', 'projection.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Model.from_pretrained('Qwen/Qwen2.5-7B-Instruct', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04701964",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder = model.encoder.from_pretrained('huseinzol05/whisper-large-v3-encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2261600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38fff53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_token = \"<|file_sep|>\"\n",
    "audio_bos_token = \"<|audio_bos|>\"\n",
    "audio_eos_token = \"<|audio_eos|>\"\n",
    "audio_token_id = tokenizer._convert_token_to_id_with_added_voc(audio_token)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "new_tokens = [AddedToken(audio_bos_token), AddedToken(audio_eos_token)]\n",
    "tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73425033",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.audio_token_index = audio_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d6d1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nAudio 1: <|audio_bos|><|file_sep|><|audio_eos|>\\nWhat does the person say?<|im_end|>\\n<|im_start|>assistant\\nYes, the speaker is female and in her twenties.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": \"audio.wav\"},\n",
    "        {\"type\": \"text\", \"text\": \"What does the person say?\"},\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46118f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_class = Audio(sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa179fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'line-4.mp3'\n",
    "audio_ = audio_class.decode_example(audio_class.encode_example(f))['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db68f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_lengths = [min(3000, math.ceil(len(audio_) / processor.feature_extractor.hop_length))]\n",
    "audio_length = audio_lengths.pop(0)\n",
    "input_length = (audio_length - 1) // 2 + 1\n",
    "\n",
    "expanded_audio_token = audio_token * input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18cc546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(audio_token, expanded_audio_token)\n",
    "inputs = tokenizer(text, return_tensors = 'pt')\n",
    "input_ids = inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbb8d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_audio = processor.feature_extractor(\n",
    "    [audio_], \n",
    "    return_attention_mask=True, \n",
    "    padding=\"max_length\", \n",
    "    sampling_rate=16000,\n",
    "    return_tensors = 'pt'\n",
    ")\n",
    "\n",
    "input_features = inputs_audio['input_features']\n",
    "feature_attention_mask = inputs_audio['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be2b86a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPast(last_hidden_state=tensor([[[-0.1157,  0.1346,  1.1895,  ..., -0.8239,  0.3654,  1.1142],\n",
       "         [ 0.3169,  0.7667, -0.2151,  ..., -4.4913,  3.2864,  4.9101],\n",
       "         [-0.2525,  0.3457, -3.7474,  ...,  0.9931, -0.0381,  3.4437],\n",
       "         ...,\n",
       "         [ 5.2484,  2.1452, -1.2385,  ..., -1.6588, -0.3712,  0.3865],\n",
       "         [ 0.2862,  4.0409, -1.9994,  ..., -3.8556,  0.6829,  1.1654],\n",
       "         [ 1.3507,  7.6016,  5.0270,  ..., -0.5609, -1.1688, -1.6081]]],\n",
       "       grad_fn=<MulBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x7faef00c71c0>, hidden_states=(tensor([[[ 1.2398e-04, -9.0599e-05,  1.2064e-04,  ..., -2.3842e-04,\n",
       "          -2.8491e-05, -1.6785e-04],\n",
       "         [-6.7520e-04, -8.3618e-03, -1.0376e-03,  ..., -3.0273e-02,\n",
       "          -1.8433e-02, -7.6904e-03],\n",
       "         [ 1.4572e-03, -4.5776e-03, -8.4229e-03,  ..., -2.0447e-03,\n",
       "          -8.9264e-04, -6.8970e-03],\n",
       "         ...,\n",
       "         [ 1.2398e-04, -9.0599e-05,  1.2064e-04,  ..., -2.3842e-04,\n",
       "          -2.8491e-05, -1.6785e-04],\n",
       "         [ 1.0620e-02, -1.2024e-02,  3.2654e-03,  ..., -2.0386e-02,\n",
       "          -1.2817e-02,  1.6968e-02],\n",
       "         [ 1.4572e-03, -4.5776e-03, -8.4229e-03,  ..., -2.0447e-03,\n",
       "          -8.9264e-04, -6.8970e-03]]], grad_fn=<IndexPutBackward0>), tensor([[[-0.1067,  0.0537, -0.3166,  ..., -0.0558, -0.0466,  0.0450],\n",
       "         [-0.1590,  0.0013, -0.2705,  ..., -0.0817, -0.1567, -0.0295],\n",
       "         [-0.0215,  0.1094,  0.0654,  ...,  0.0983, -0.1311, -0.0687],\n",
       "         ...,\n",
       "         [-0.0693,  0.0506, -0.1121,  ...,  0.0280, -0.0418, -0.0033],\n",
       "         [-0.1164, -0.0573, -0.2591,  ..., -0.0829, -0.0271,  0.0026],\n",
       "         [-0.0841, -0.1497, -0.0851,  ..., -0.0277, -0.0887, -0.0422]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-1.9126, -2.0371, -0.4291,  ...,  0.2650,  0.5287,  1.6870],\n",
       "         [-0.2220, -0.1096, -0.0891,  ..., -0.3784, -0.1734,  0.1019],\n",
       "         [ 0.0311,  0.0943, -0.0171,  ...,  0.0496, -0.3000, -0.1138],\n",
       "         ...,\n",
       "         [ 0.0478,  0.0127, -0.1936,  ..., -0.1215, -0.0380,  0.0026],\n",
       "         [-0.1666, -0.0426, -0.3052,  ..., -0.0929, -0.0340,  0.0116],\n",
       "         [ 0.0195, -0.1370, -0.1807,  ..., -0.0867, -0.1483, -0.0574]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-2.4389,  0.2518, -2.6725,  ...,  0.1568,  1.5317,  0.1651],\n",
       "         [-0.3806, -0.1375,  0.0331,  ..., -0.3566,  0.1591, -0.2562],\n",
       "         [ 0.0428,  0.2947,  0.2406,  ..., -0.0092, -0.0951,  0.0469],\n",
       "         ...,\n",
       "         [ 0.0665, -0.0423, -0.0116,  ..., -0.2455,  0.0186,  0.0342],\n",
       "         [-0.2152, -0.8467,  0.5077,  ..., -0.0967, -0.6093,  0.0639],\n",
       "         [-0.0255, -0.0874, -0.1551,  ..., -0.1056, -0.1181, -0.0993]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-2.7412, -1.1701, -1.6512,  ..., -1.6554,  1.8960,  1.4372],\n",
       "         [-0.8177, -0.3322,  0.2938,  ..., -0.7349, -0.0195, -0.0243],\n",
       "         [-4.0033, 14.6118, -6.1245,  ..., -0.0619,  3.3843,  8.4826],\n",
       "         ...,\n",
       "         [ 0.0599, -0.0278, -0.0793,  ..., -0.1744, -0.0562,  0.0694],\n",
       "         [-0.1705, -0.6925, -0.0465,  ...,  0.1936, -0.6220,  0.3742],\n",
       "         [-0.0932, -0.0467, -0.0655,  ..., -0.0230, -0.0976, -0.0924]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-1.7827,  1.0300, -2.8472,  ..., -7.4898,  1.5024,  2.1981],\n",
       "         [-1.0939, -0.7639,  0.2635,  ..., -1.0434, -0.7912,  2.0169],\n",
       "         [-3.7422, 10.0744, -5.9083,  ..., -2.7406,  7.6519,  3.8912],\n",
       "         ...,\n",
       "         [-0.1606, -0.1436,  0.0445,  ..., -0.2303,  0.0181, -0.2582],\n",
       "         [-0.5521, -1.1159,  0.0156,  ..., -0.1874, -0.6167,  0.0510],\n",
       "         [-0.2955, -0.0674,  0.0897,  ..., -0.0920, -0.1375,  0.3124]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-2.1222e+00,  1.8553e-03,  1.9158e+00,  ..., -8.7819e+00,\n",
       "           2.8110e+00,  4.0614e-01],\n",
       "         [-1.2462e+00, -1.8619e-01, -3.2511e-01,  ..., -1.3508e+00,\n",
       "          -9.7032e-01,  9.4287e-01],\n",
       "         [-4.9244e+00,  8.2282e+00, -6.8887e+00,  ..., -1.2015e+00,\n",
       "           6.6229e+00,  4.0711e+00],\n",
       "         ...,\n",
       "         [-2.8989e-02,  7.5274e-03, -1.0686e-01,  ..., -2.5055e-01,\n",
       "          -1.0853e-01, -1.6999e-01],\n",
       "         [-2.5085e-01, -7.2237e-01, -1.9141e-01,  ..., -2.8175e-01,\n",
       "          -4.1758e-01,  3.7484e-02],\n",
       "         [-2.3960e-01, -1.2831e-01, -2.9036e-01,  ...,  9.2086e-02,\n",
       "           5.7906e-02,  3.9006e-01]]], grad_fn=<AddBackward0>), tensor([[[-4.5433, -1.0857,  3.5172,  ..., -2.9770,  0.3077,  0.0313],\n",
       "         [-1.2844, -0.2783, -0.3028,  ..., -0.8184, -0.7949,  1.6671],\n",
       "         [-4.4853,  6.8724, -5.4026,  ..., -0.3964,  6.8706,  3.5441],\n",
       "         ...,\n",
       "         [ 0.0233,  0.0826,  0.1115,  ..., -0.2863, -0.4311, -0.0228],\n",
       "         [-0.0165, -0.7581, -0.1295,  ..., -0.3388, -0.1318,  0.4867],\n",
       "         [-0.3048, -0.4205,  0.1600,  ...,  0.0119,  0.1867,  0.3917]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-5.0145, -1.5916,  2.6195,  ..., -2.4319,  0.0540, -0.7746],\n",
       "         [-1.5218,  0.0878, -0.3661,  ..., -2.0684, -0.6913,  1.4693],\n",
       "         [-4.0788,  6.4314, -5.4378,  ..., -0.3411,  6.4225,  3.0334],\n",
       "         ...,\n",
       "         [ 0.1769, -0.0659, -0.0765,  ...,  0.1614, -0.0951,  0.0367],\n",
       "         [ 0.2438, -0.7661,  0.1686,  ..., -0.7348, -0.6348, -0.0494],\n",
       "         [-0.1993, -0.4172,  0.1694,  ..., -0.5101, -0.1932,  0.0434]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-4.8721, -1.4896,  1.9149,  ..., -1.4224,  0.1440, -1.1549],\n",
       "         [-0.8519,  0.4214,  0.2355,  ..., -1.9351, -0.4868,  1.0329],\n",
       "         [-4.0796,  6.8767, -4.8930,  ..., -0.1680,  6.5476,  2.2160],\n",
       "         ...,\n",
       "         [ 0.3001,  0.3419,  0.3539,  ..., -0.1616,  0.0179,  0.3554],\n",
       "         [ 0.3386, -0.9220,  0.0824,  ..., -0.8525, -0.2974, -0.1186],\n",
       "         [-0.0569, -0.5632,  0.0573,  ..., -0.7697, -0.1495,  0.1576]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-5.5504, -0.2101,  3.2636,  ..., -2.2030, -0.2964, -2.4567],\n",
       "         [-0.6732,  0.7118,  0.0506,  ..., -1.9057, -0.4387,  0.8984],\n",
       "         [-4.1609,  4.8412, -4.5479,  ..., -0.5764,  7.5779,  0.4339],\n",
       "         ...,\n",
       "         [ 0.1566,  0.4283,  0.4042,  ..., -0.6869, -0.1346, -0.2955],\n",
       "         [-0.1041, -0.2229, -0.0947,  ..., -1.1485,  0.0400, -0.0495],\n",
       "         [ 0.4465,  0.1712, -0.0542,  ..., -0.8644, -0.2442, -0.1852]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-4.4327e+00,  3.0861e-01,  3.8509e+00,  ..., -2.2189e+00,\n",
       "           4.9922e-01, -2.4895e+00],\n",
       "         [-5.1381e-01,  4.2533e-01, -1.9750e-01,  ..., -2.0302e+00,\n",
       "          -1.8547e-03,  1.1532e+00],\n",
       "         [-4.0645e+00,  4.6991e+00, -4.8221e+00,  ..., -2.6669e-01,\n",
       "           7.4836e+00,  6.5169e-01],\n",
       "         ...,\n",
       "         [ 1.8454e-02, -2.4475e-02,  2.2954e-01,  ..., -5.0856e-01,\n",
       "          -3.5134e-01, -1.3959e-01],\n",
       "         [-1.3187e-01,  1.9681e-01, -1.0751e-01,  ..., -5.9558e-01,\n",
       "           2.9977e-02, -2.2400e-01],\n",
       "         [-6.0789e-02,  2.5057e-01, -1.6128e-01,  ..., -3.0617e-01,\n",
       "          -3.5370e-01,  1.7524e-01]]], grad_fn=<AddBackward0>), tensor([[[-3.5510e+00,  1.3128e+00,  3.3261e+00,  ..., -2.2604e+00,\n",
       "           1.2274e-01, -1.5677e+00],\n",
       "         [ 6.7198e-02,  7.6317e-01,  2.4396e-01,  ..., -1.4420e+00,\n",
       "          -1.1399e-01,  8.3403e-01],\n",
       "         [-3.9735e+00,  5.0770e+00, -3.7119e+00,  ...,  6.0402e-01,\n",
       "           7.3396e+00,  4.1533e-01],\n",
       "         ...,\n",
       "         [ 7.8132e-02, -1.5542e-01, -1.3278e-01,  ..., -2.3268e-01,\n",
       "           2.5483e-02,  4.9482e-01],\n",
       "         [ 1.7721e-01,  3.6519e-02, -5.4498e-01,  ..., -1.3088e+00,\n",
       "          -5.9595e-02, -1.6385e-01],\n",
       "         [ 8.8520e-02,  4.0435e-01, -6.7367e-03,  ..., -2.5978e-01,\n",
       "           9.7778e-02,  1.5252e-02]]], grad_fn=<AddBackward0>), tensor([[[-2.7250,  2.2881,  2.7703,  ..., -1.9865,  0.2456, -1.3039],\n",
       "         [ 0.0608,  0.2520,  0.7787,  ..., -1.4915, -0.0264,  0.7188],\n",
       "         [-4.2546,  4.2427, -3.3456,  ...,  1.3564,  6.8101,  0.2191],\n",
       "         ...,\n",
       "         [-0.0180, -0.0302,  0.1033,  ..., -0.1742,  0.1272,  0.3576],\n",
       "         [ 0.1975, -0.2588,  0.2601,  ..., -1.0085,  0.1451, -0.1438],\n",
       "         [ 0.2411,  0.0795,  0.8382,  ..., -0.2324,  0.1247,  0.2897]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-1.8428e+00,  2.3767e+00,  2.6511e+00,  ..., -1.7522e+00,\n",
       "           2.8489e-01, -6.8354e-01],\n",
       "         [ 5.2786e-02,  7.3632e-01,  7.7126e-01,  ..., -1.4527e+00,\n",
       "           1.8383e-01,  1.9844e-01],\n",
       "         [-3.9934e+00,  3.8042e+00, -3.9233e+00,  ...,  1.6977e+00,\n",
       "           6.6690e+00,  5.5377e-01],\n",
       "         ...,\n",
       "         [-1.8717e-01,  3.0650e-01,  3.2902e-01,  ..., -2.3846e-01,\n",
       "          -1.8117e-01,  1.1213e-02],\n",
       "         [-7.0491e-02, -4.4809e-01, -4.7174e-01,  ..., -8.8734e-01,\n",
       "          -3.9499e-01,  7.5646e-01],\n",
       "         [-4.7035e-02, -4.0933e-01, -3.8267e-03,  ..., -5.2436e-01,\n",
       "          -4.0172e-01,  3.4923e-01]]], grad_fn=<AddBackward0>), tensor([[[-0.9264,  2.0383,  3.7135,  ..., -1.9024,  0.1813, -0.3463],\n",
       "         [ 0.0196,  0.2140,  1.1613,  ..., -2.1905, -0.2088,  0.4022],\n",
       "         [-3.6356,  3.8459, -3.8655,  ...,  2.0656,  6.7318,  0.0803],\n",
       "         ...,\n",
       "         [-0.1637, -0.4326,  0.3052,  ..., -0.3849, -0.2534, -0.5551],\n",
       "         [-0.2171,  0.0110,  0.2005,  ..., -0.6546,  0.3731,  0.2743],\n",
       "         [-0.0288, -0.6326,  0.7847,  ..., -0.4460,  0.1316, -0.1717]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-1.2443e+00,  2.3923e+00,  2.8559e+00,  ..., -2.0778e+00,\n",
       "          -9.2028e-01, -5.3502e-01],\n",
       "         [ 7.5824e-02,  8.2767e-01,  2.9975e-01,  ..., -2.5110e+00,\n",
       "           1.4246e-01,  7.9876e-02],\n",
       "         [-3.7272e+00,  3.6014e+00, -3.5012e+00,  ...,  2.0998e+00,\n",
       "           6.8708e+00, -1.8245e-01],\n",
       "         ...,\n",
       "         [-6.1349e-01, -2.3532e-01, -5.0095e-02,  ..., -4.4637e-01,\n",
       "           6.2608e-02, -7.9779e-01],\n",
       "         [-5.7390e-01,  5.4758e-03, -4.6695e-01,  ..., -1.0926e-01,\n",
       "           7.4181e-02,  3.9632e-01],\n",
       "         [-5.2364e-01, -6.2340e-01, -5.4221e-02,  ...,  1.1317e-01,\n",
       "           6.0840e-01, -7.7673e-02]]], grad_fn=<AddBackward0>), tensor([[[-1.4753,  1.2687,  2.8218,  ..., -1.3977, -1.3504, -1.8422],\n",
       "         [ 0.3146,  0.7359,  0.4159,  ..., -3.0869,  0.0451,  0.5479],\n",
       "         [-3.4380,  3.3615, -3.0646,  ...,  2.1365,  7.0694, -0.2739],\n",
       "         ...,\n",
       "         [-0.3537, -0.5325,  0.1970,  ..., -1.1470, -0.0583, -0.6422],\n",
       "         [-0.8149, -1.4206,  0.5451,  ..., -0.2909, -0.5257, -0.0221],\n",
       "         [-0.4847, -1.3525, -0.5159,  ...,  0.1351,  0.0145, -0.7285]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-1.7807,  1.3173,  2.6520,  ..., -0.7844, -0.7451, -1.5583],\n",
       "         [ 0.7394,  0.3521,  0.8517,  ..., -2.8872, -0.3483,  0.5071],\n",
       "         [-3.2840,  3.1107, -2.6194,  ...,  2.1184,  7.3232, -0.2060],\n",
       "         ...,\n",
       "         [-0.2980, -0.2840, -0.4975,  ..., -0.9770, -0.6051, -0.7091],\n",
       "         [-0.3281, -0.8603,  0.9139,  ...,  0.0952, -0.0112, -0.0199],\n",
       "         [-0.1623, -0.9737, -0.0899,  ...,  0.0225, -1.0184, -0.4210]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-1.7949,  0.9500,  2.3516,  ..., -1.2482,  0.2858, -1.2778],\n",
       "         [ 0.6793,  0.7814, -0.0397,  ..., -3.7646, -0.2264, -0.0113],\n",
       "         [-2.9533,  2.8996, -1.7860,  ...,  2.6279,  7.2518,  0.0580],\n",
       "         ...,\n",
       "         [-0.6882,  0.1855, -0.9513,  ..., -1.4951, -0.5912, -0.4876],\n",
       "         [-0.4056, -0.8300,  0.7130,  ..., -0.8331, -0.7622,  0.7051],\n",
       "         [-0.3564, -1.5439,  0.3028,  ..., -0.0931, -0.7076, -0.3210]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-0.8157,  0.6136,  1.8924,  ..., -1.3004,  0.7550, -2.0288],\n",
       "         [ 0.1432,  0.3346,  0.0310,  ..., -4.0350, -0.0624, -0.6473],\n",
       "         [-2.7488,  2.0258, -0.9180,  ...,  2.8812,  7.2740,  0.1212],\n",
       "         ...,\n",
       "         [ 0.1988,  0.1522, -0.7751,  ..., -1.4263, -0.6410, -0.3086],\n",
       "         [ 0.4256,  0.0597,  1.8047,  ..., -0.1884,  0.0564,  0.2704],\n",
       "         [ 0.2269, -0.6228,  0.1055,  ..., -0.4461, -0.3196, -0.3783]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-7.7650e-01,  6.0719e-01,  8.4336e-01,  ..., -2.0106e-01,\n",
       "           4.4740e-01, -1.8204e+00],\n",
       "         [ 3.3324e-01, -8.1364e-03, -1.4562e+00,  ..., -3.9410e+00,\n",
       "          -1.7152e-01, -8.3150e-01],\n",
       "         [-2.9135e+00,  1.2343e+00, -1.2733e+00,  ...,  3.0486e+00,\n",
       "           7.2761e+00,  1.4693e-01],\n",
       "         ...,\n",
       "         [-1.3005e-01,  8.5034e-01, -2.0232e+00,  ..., -5.9161e-01,\n",
       "          -1.2991e+00, -3.3017e-01],\n",
       "         [-4.7230e-03, -2.7829e-02,  3.4733e-01,  ...,  2.6642e-01,\n",
       "           2.9739e-01,  9.0421e-01],\n",
       "         [-2.0789e-01,  6.6752e-01,  2.0180e-01,  ..., -1.0761e+00,\n",
       "          -8.4425e-01, -5.1113e-01]]], grad_fn=<AddBackward0>), tensor([[[-1.3771e+00,  2.0503e-01, -4.8733e-01,  ...,  7.8410e-01,\n",
       "           2.1738e+00, -2.3519e-01],\n",
       "         [-4.2139e-01,  5.1439e-02, -2.2408e-01,  ..., -3.5520e+00,\n",
       "           1.0923e-01,  1.3757e-01],\n",
       "         [-3.3739e+00,  1.1606e-01, -5.3900e-01,  ...,  3.2785e+00,\n",
       "           7.1877e+00, -1.4471e-01],\n",
       "         ...,\n",
       "         [ 8.2332e-01, -4.0408e-01, -9.9723e-01,  ...,  2.0641e-01,\n",
       "          -4.2534e-03, -1.4357e+00],\n",
       "         [ 1.6337e+00,  4.6250e-01, -9.9532e-01,  ...,  2.8023e-01,\n",
       "           1.0236e+00, -1.4705e-01],\n",
       "         [ 2.4996e-02,  9.6082e-01,  3.8101e-02,  ..., -9.1149e-01,\n",
       "          -5.0286e-01, -2.0119e+00]]], grad_fn=<AddBackward0>), tensor([[[-0.7257, -0.2858,  1.3161,  ..., -0.5259,  2.5745, -0.0941],\n",
       "         [-1.3814, -2.4360,  0.8474,  ..., -5.3921, -0.9837,  1.2255],\n",
       "         [-3.8674, -0.9671, -0.0603,  ...,  3.6740,  6.2318, -1.2506],\n",
       "         ...,\n",
       "         [ 0.1155,  0.9325, -1.4042,  ...,  0.3479, -1.8133, -0.6234],\n",
       "         [ 1.1960,  0.3125, -0.6598,  ..., -1.0279,  0.5262,  0.7514],\n",
       "         [-0.9244, -1.0600, -0.5616,  ..., -0.4724, -0.8426, -1.1055]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[ 1.3619,  0.3577,  2.5460,  ...,  1.1157,  2.2209,  0.2191],\n",
       "         [ 0.0478, -2.0803,  2.2198,  ..., -4.9389, -0.5748,  1.8924],\n",
       "         [-4.2912, -2.1126,  1.4338,  ...,  4.3719,  5.8966, -1.1847],\n",
       "         ...,\n",
       "         [ 1.1255, -0.4417, -1.2667,  ..., -0.4922, -0.4149, -1.6655],\n",
       "         [ 2.0784,  1.6627,  0.0626,  ..., -1.8670,  2.2919, -0.3573],\n",
       "         [-0.3376,  2.0799,  0.1167,  ..., -0.8029, -0.8365, -1.2113]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[ 1.2758,  0.0207,  3.0143,  ...,  0.1491,  2.6136,  1.0742],\n",
       "         [-0.8938, -1.3446,  1.7682,  ..., -4.8869,  0.4493,  0.9261],\n",
       "         [-4.2746, -3.0720,  2.6086,  ...,  6.6693,  4.7221, -3.1316],\n",
       "         ...,\n",
       "         [-0.4430, -0.9373, -1.4652,  ..., -0.2109,  0.9663, -2.7473],\n",
       "         [ 2.3328,  1.5356,  0.3990,  ..., -2.3343, -0.0254, -2.2975],\n",
       "         [-0.9214,  2.9297,  2.0211,  ...,  0.1926, -0.1988, -2.6834]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-0.5139,  1.5141,  2.1277,  ..., -0.5918,  2.9975,  0.8977],\n",
       "         [ 0.3593,  0.9814, -1.3118,  ..., -6.0539,  1.9776,  0.9424],\n",
       "         [-1.8723,  0.5928,  2.8207,  ...,  7.7350,  3.5879, -4.4128],\n",
       "         ...,\n",
       "         [-1.4738,  0.2930, -1.0306,  ..., -0.6119,  1.7768, -2.1533],\n",
       "         [ 1.2595,  2.6283, -0.0431,  ..., -0.6960, -1.5728, -1.8369],\n",
       "         [-0.6058,  5.3549,  3.1388,  ..., -0.8251, -0.1488, -1.2599]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[ -0.3482,  -0.9900,   6.4564,  ...,  -1.6846,   1.8866,  -1.5798],\n",
       "         [ -2.2973,   0.4131,  -0.3242,  ...,  -7.1029,   4.5977,   8.9723],\n",
       "         [ -8.8783, -12.5585,  14.2286,  ..., -15.2574,  -9.8568,  -0.2785],\n",
       "         ...,\n",
       "         [  4.4585,  -1.1130,  -2.4672,  ...,   3.1009,  -0.2835,  -3.4709],\n",
       "         [ -0.3302,   1.8371,  -0.4345,  ...,   0.1700,  -3.0310,   0.2058],\n",
       "         [ -0.7633,   6.7076,   3.5112,  ...,  -0.7038,  -2.5110,  -4.7722]]],\n",
       "       grad_fn=<AddBackward0>), tensor([[[-0.1157,  0.1346,  1.1895,  ..., -0.8239,  0.3654,  1.1142],\n",
       "         [ 0.3169,  0.7667, -0.2151,  ..., -4.4913,  3.2864,  4.9101],\n",
       "         [-0.2525,  0.3457, -3.7474,  ...,  0.9931, -0.0381,  3.4437],\n",
       "         ...,\n",
       "         [ 5.2484,  2.1452, -1.2385,  ..., -1.6588, -0.3712,  0.3865],\n",
       "         [ 0.2862,  4.0409, -1.9994,  ..., -3.8556,  0.6829,  1.1654],\n",
       "         [ 1.3507,  7.6016,  5.0270,  ..., -0.5609, -1.1688, -1.6081]]],\n",
       "       grad_fn=<MulBackward0>)), attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    input_ids = input_ids, \n",
    "    attention_mask = inputs['attention_mask'],\n",
    "    input_features = input_features,\n",
    "    feature_attention_mask = feature_attention_mask,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
