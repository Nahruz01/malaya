{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c36cd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mesolitica/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-14B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b234a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import streaming\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6850df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tokenized-8k-qwen\n",
    "!mkdir tokenized-8k-qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359d9cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1327479"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine = []\n",
    "with open('combined-malaysian-sft.jsonl') as fopen:\n",
    "    for l in fopen:\n",
    "        l = json.loads(l)\n",
    "        combine.append(l)\n",
    "\n",
    "len(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72e4ae1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397479"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('translation-instructions.json') as fopen:\n",
    "    translation = json.load(fopen)\n",
    "    \n",
    "for d in translation:\n",
    "    combine.append([\n",
    "        {'role': 'user', 'content': d['input']},\n",
    "        {'role': 'assistant', 'content': d['output']}\n",
    "    ])\n",
    "    \n",
    "len(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6518f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = combine[-1]\n",
    "prompt = tokenizer.apply_chat_template(row, tokenize=False)\n",
    "outputs = tokenizer(prompt, add_special_tokens = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "067fa800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "    }\n",
    "\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first = []\n",
    "    balance = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remaining_space = size - current_size\n",
    "            if len(sublist) <= remaining_space:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remaining_space])\n",
    "                balance.append(sublist[remaining_space:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    \n",
    "    return first, balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5638b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def loop(files, block_size = 8192):\n",
    "    rows, index = files\n",
    "    out_root = f'tokenized-8k-qwen/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "            prompt = tokenizer.apply_chat_template(row, tokenize=False)\n",
    "            outputs = tokenizer(prompt, add_special_tokens = False)\n",
    "            temp.append(outputs['input_ids'])\n",
    "            position_ids.append(range(len(outputs['input_ids'])))\n",
    "            count += len(outputs['input_ids'])\n",
    "            while count >= block_size:\n",
    "                block, temp = slice_and_balance(temp, block_size)\n",
    "                block_position, position_ids = slice_and_balance(position_ids, block_size)\n",
    "                count = count - block_size\n",
    "                o = collator(block, block_position)\n",
    "                last_block = block\n",
    "                last_position_block = block_position\n",
    "                out.write(o)\n",
    "                \n",
    "        block, _ = slice_and_balance(last_block, block_size - count)\n",
    "        block_position, _ = slice_and_balance(last_position_block, block_size - count)\n",
    "\n",
    "        block.extend(temp)\n",
    "        block_position.extend(position_ids)\n",
    "\n",
    "        o = collator(block, block_position)\n",
    "        if len(o['input_ids']) == block_size:\n",
    "            out.write(o)\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "510a18bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 864.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([  1833,  64907,     88, ...,     13, 151645,    198], dtype=uint32),\n",
       " 'position_ids': array([161, 162, 163, ..., 357, 358, 359], dtype=uint32),\n",
       " 'attention_mask': array([468, 448, 479, 375, 542, 486, 635, 286, 614, 545, 864, 742, 335,\n",
       "        480, 677,  72, 144], dtype=uint32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop((combine[:1000], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8afbc054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:37<00:00, 1338.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:25<00:00, 1998.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:18<00:00, 2665.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:26<00:00, 579.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:07<00:00, 736.96it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:18<00:00, 2652.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:48<00:00, 462.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:24<00:00, 590.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:33<00:00, 1512.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:04<00:00, 402.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:25<00:00, 344.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:34<00:00, 324.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:26<00:00, 576.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:56<00:00, 283.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:45<00:00, 472.08it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [03:24<00:00, 244.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:04<00:00, 403.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:04<00:00, 403.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:23<00:00, 596.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:35<00:00, 525.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:35<00:00, 320.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 47479/47479 [00:39<00:00, 1189.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:31<00:00, 329.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:56<00:00, 877.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:33<00:00, 533.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:13<00:00, 375.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [02:59<00:00, 277.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50000/50000 [03:51<00:00, 215.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from multiprocess import Pool\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (l[i: i + n], i // n)\n",
    "\n",
    "chunks = chunks(combine, 50000)\n",
    "pool = Pool(10)\n",
    "pooled = pool.map(loop, chunks)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7134e40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenized-8k-qwen/tokenized-0',\n",
       " 'tokenized-8k-qwen/tokenized-1',\n",
       " 'tokenized-8k-qwen/tokenized-2',\n",
       " 'tokenized-8k-qwen/tokenized-3',\n",
       " 'tokenized-8k-qwen/tokenized-4',\n",
       " 'tokenized-8k-qwen/tokenized-5',\n",
       " 'tokenized-8k-qwen/tokenized-6',\n",
       " 'tokenized-8k-qwen/tokenized-7',\n",
       " 'tokenized-8k-qwen/tokenized-8',\n",
       " 'tokenized-8k-qwen/tokenized-9',\n",
       " 'tokenized-8k-qwen/tokenized-10',\n",
       " 'tokenized-8k-qwen/tokenized-11',\n",
       " 'tokenized-8k-qwen/tokenized-12',\n",
       " 'tokenized-8k-qwen/tokenized-13',\n",
       " 'tokenized-8k-qwen/tokenized-14',\n",
       " 'tokenized-8k-qwen/tokenized-15',\n",
       " 'tokenized-8k-qwen/tokenized-16',\n",
       " 'tokenized-8k-qwen/tokenized-17',\n",
       " 'tokenized-8k-qwen/tokenized-18',\n",
       " 'tokenized-8k-qwen/tokenized-19',\n",
       " 'tokenized-8k-qwen/tokenized-20',\n",
       " 'tokenized-8k-qwen/tokenized-21',\n",
       " 'tokenized-8k-qwen/tokenized-22',\n",
       " 'tokenized-8k-qwen/tokenized-23',\n",
       " 'tokenized-8k-qwen/tokenized-24',\n",
       " 'tokenized-8k-qwen/tokenized-25',\n",
       " 'tokenized-8k-qwen/tokenized-26',\n",
       " 'tokenized-8k-qwen/tokenized-27']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = sorted(glob('tokenized-8k-qwen/tokenized-*'), key = lambda x: int(x.split('-')[-1]))\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "735902a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf packing-8k-qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "735c6035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 4198/4198 [00:00<00:00, 5033.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 1547/1547 [00:00<00:00, 7048.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 8934/8934 [00:01<00:00, 5024.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 5087/5087 [00:01<00:00, 5016.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 5585/5585 [00:01<00:00, 5446.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 6521/6521 [00:01<00:00, 4882.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 9833/9833 [00:01<00:00, 5451.16it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 8839/8839 [00:01<00:00, 4977.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 3974/3974 [00:00<00:00, 4988.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 3077/3077 [00:00<00:00, 5170.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 978/978 [00:00<00:00, 5183.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 656/656 [00:00<00:00, 23826.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 658/658 [00:00<00:00, 3715.83it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 1220/1220 [00:00<00:00, 6139.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 4180/4180 [00:00<00:00, 5354.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 5444/5444 [00:01<00:00, 4865.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 4532/4532 [00:00<00:00, 5763.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 5277/5277 [00:00<00:00, 5487.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 6533/6533 [00:01<00:00, 4942.33it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 6293/6293 [00:01<00:00, 5315.91it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 4021/4021 [00:00<00:00, 5298.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 3476/3476 [00:00<00:00, 5898.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 7348/7348 [00:01<00:00, 5360.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 3949/3949 [00:00<00:00, 5008.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 7126/7126 [00:01<00:00, 5317.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 12967/12967 [00:02<00:00, 5131.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 2175/2175 [00:00<00:00, 5645.84it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 1707/1707 [00:00<00:00, 4649.34it/s]\n"
     ]
    }
   ],
   "source": [
    "with MDSWriter(out='packing-8k-qwen', columns=columns, compression=None, hashes=hashes) as out:\n",
    "    for f in folders:\n",
    "        try:\n",
    "            dataset = LocalDataset(local=f)\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                out.write(dataset[i])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff08495b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.11521792"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LocalDataset('packing-8k-qwen')\n",
    "(len(dataset) * 8192) / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6099b9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' mental, hubungan sosial, dan waktu untuk mengejar minat dan hobi pribadi. \\n\\nUntuk dapat mencapai keseimbangan yang baik antara pekerjaan dan kehidupan, penting untuk menetapkan prioritas dan membangun rutinitas yang sehat. Berikut adalah beberapa tips yang dapat membantu:\\n\\n1. Tetapkan batasan waktu kerja: Tetapkan jam kerja yang tepat dan usahakan untuk menepatinya. Jangan membawa pekerjaan ke rumah atau memeriksa email kerja di waktu-waktu yang seharusnya untuk bersantai.\\n\\n2. Buat jadwal rutin: Buat jadwal rutin yang mencakup waktu untuk pekerjaan, istirahat, aktivitas fisik, dan kegiatan sosial. Tetapkan waktu untuk memasak, belanja, dan rutinitas rumah lainnya agar tidak terbebani selama hari kerja.\\n\\n3. Prioritaskan kesehatan fisik: Selalu usahakan mengatur jadwal rutin untuk olahraga dan waktu istirahat agar tidak terlalu capek karena pekerjaan. Ini akan membantu menjaga kesehatan fisik dan juga memberi waktu untuk merenung dan merilekskan diri.\\n\\n4. Gunakan alat bantu: Menggunakan alat bantu seperti aplikasi produktivitas dan manajemen waktu dapat membantu untuk mengelola pekerjaan dan memastikan waktu yang cukup tersedia untuk kegiatan lainnya.\\n\\n5. Berbicara dengan atasan dan kolega: Selalu ingat bahwa itu adalah hak setiap karyawan untuk memiliki keseimbangan yang sehat antara pekerjaan dan kehidupan pribadi. Jangan ragu untuk berbicara dengan atasan dan rekan kerja untuk mengatur jadwal yang memungkinkan Anda memprioritaskan kehidupan pribadi Anda. \\n\\nDengan menetapkan prioritas, membangun rutinitas rutin, dan berbicara dengan atasan dan kolega, seseorang dapat mencapai keseimbangan yang baik antara pekerjaan dan kehidupan pribadi mereka. Ini dapat membantu meningkatkan kesejahteraan secara keseluruhan, memberi seseorang waktu dan ruang untuk mengejar minat dan hobi yang lebih kreatif, dan meningkatkan produktivitas di tempat kerja.\\nterjemah ke melayu baku<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nKeseimbangan antara kerja dan kehidupan merujuk kepada keupayaan seseorang untuk mengekalkan keseimbangan yang sihat antara tuntutan kerja dan kehidupan peribadi mereka. Ini termasuk masa yang dihabiskan untuk bekerja, kesihatan fizikal dan mental, hubungan sosial, dan masa untuk mengejar minat dan hobi peribadi.\\n\\nUntuk mencapai keseimbangan yang baik antara pekerjaan dan kehidupan, adalah penting untuk menetapkan keutamaan dan membina rutin yang sihat. Berikut ialah beberapa petua yang boleh membantu:\\n\\n1. Tetapkan had waktu bekerja: Tetapkan jam kerja yang betul dan cuba untuk menepatinya. Jangan membawa kerja ke rumah atau menyemak e-mel kerja pada waktu yang sepatutnya untuk berehat.\\n\\n2. Buat jadual rutin: Buat jadual rutin yang merangkumi masa untuk bekerja, rehat, aktiviti fizikal dan aktiviti sosial. Tetapkan masa untuk memasak, membeli-belah dan rutin lain di rumah supaya anda tidak terbeban semasa hari kerja.\\n\\n3. Utamakan kesihatan fizikal: Sentiasa berusaha untuk mengatur jadual rutin untuk bersenam dan waktu rehat agar anda tidak terlalu letih kerana kerja. Ini akan membantu menjaga kesihatan fizikal dan juga memberi masa untuk merenung dan melegakan diri.\\n\\n4. Gunakan alat bantu: Menggunakan alat seperti aplikasi produktiviti dan pengurusan masa dapat membantu menguruskan kerja dan memastikan masa yang mencukupi tersedia untuk aktiviti lain.\\n\\n5. Bercakap dengan bos dan rakan sekerja: Sentiasa ingat bahawa ia adalah hak setiap pekerja untuk mempunyai keseimbangan yang sihat antara pekerjaan dan kehidupan peribadi. Jangan teragak-agak untuk bercakap dengan bos dan rakan sekerja untuk menyusun jadual yang membolehkan anda mengutamakan kehidupan peribadi anda.\\n\\nDengan menetapkan keutamaan, membina rutin rutin, dan berbincang dengan bos dan rakan sekerja, seseorang boleh mencapai keseimbangan yang baik antara pekerjaan dan kehidupan peribadi mereka. Ini boleh membantu meningkatkan kesejahteraan keseluruhan, memberi seseorang masa dan ruang untuk mengejar minat dan hobi yang lebih kreatif, serta meningkatkan produktiviti di tempat kerja.<|im_end|>\\n<|im_start|>user\\n`Kuesioner untuk mengumpulkan data tentang faktor-faktor yang memengaruhi persepsi seseorang terhadap kesenangan dan keamanan selama penerbangan termasuk namun tidak terbatas pada:\\n\\nBagian 1: Informasi Demografis\\n1. Berapa usia Anda?\\n2. Jenis kelamin Anda?\\n3. Berapa kali Anda sudah terbang sebelumnya?\\n4. Apakah Anda terbang dengan pesawat komersial atau pribadi?\\n5. Berapa lama durasi penerbangan terakhir Anda?\\n6. Tujuan Anda terbang ke mana?\\n\\nBagian 2: Susunan Tempat Duduk dan Hiburan di Dalam Pesawat\\n7. Bagaimana perasaan Anda tentang susunan tempat duduk?\\n8. Apakah Anda memiliki preferensi tempat duduk untuk membuat Anda merasa lebih aman dan nyaman?\\n9. Apakah Anda menginginkan hiburan di dalam pesawat?\\n10. Apakah Anda lebih memilih film, musik, atau permainan?\\n\\nBagian 3: Identitas Maskapai dan Reputasinya\\n11. Apakah Anda lebih memilih maskapai tertentu?\\n12. Apakah Anda merasa bahwa maskapai pilihan Anda dapat menjamin keselamatan dan kenyamanan Anda selama penerbangan?\\n13. Seberapa sering Anda terbang dengan maskapai yang sama?\\n\\nBagian 4: Kondisi Cuaca\\n14. Bagaimana Anda merasa tentang terbang dalam kondisi cuaca yang buruk?\\n15. Seberapa penting pentingnya kondisi cuaca dalam menentukan kesenangan dan keamanan penerbangan?\\n\\nBagian 5: Tujuan Perjalanan\\n16. Apakah Anda merasa bahwa tujuan perjalanan yang Anda tuju memengaruhi persepsi Anda tentang kesenangan atau keamanan penerbangan?\\n17. Apakah tujuan perjalanan Anda memperlihatkan pengaruh terhadap bagaimana Anda merasa tentang terbang?\\n\\nBagian 6: Pengalaman Terbang Sebelumnya\\n18. Seberapa sering Anda merasa gugup atau khawatir selama penerbangan?\\n19. Apakah Anda pernah mengalami kejadian yang tidak diinginkan selama penerbangan? Jika ya, apa pengalaman Anda?\\n\\nBagian 7: Pernyataan Terbang itu Menyenangkan\\n20. Terbang itu menyenangkan\\n    a. Berapa banyak Anda setuju dengan pernyataan ini dari 1-10 (1= Tidak Setuju, 10= Sangat Setuju)?\\n21. Silahkan berikan penjelasan tentang alasan Anda memberi skor dan perasaan atau pemikiran tambahan Anda yang berkaitan dengan terbang.\\n\\nTerima kasih telah berpartisipasi dalam survei kami dan waktu yang telah Anda berikan untuk mengisi kuesioner ini.` terjemah ke malay<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nSoalan untuk mengumpul data tentang faktor yang mempengaruhi persepsi seseorang terhadap keseronokan dan keselamatan semasa penerbangan termasuk tetapi tidak terhad kepada:\\n\\nBahagian 1: Maklumat Demografi\\n1. Berapakah umur anda?\\n2. Jantina anda?\\n3. Berapa kali anda telah terbang sebelum ini?\\n4. Adakah anda terbang dengan pesawat komersial atau persendirian?\\n5. Berapa lama tempoh terakhir penerbangan anda?\\n6. Ke mana anda pergi tujuan anda?\\n\\nBahagian 2: Susunan Tempat Duduk dan Hiburan Dalam Pesawat\\n7. Bagaimana perasaan anda tentang susunan tempat duduk?\\n8. Adakah anda mempunyai keutamaan tempat duduk untuk membuat anda berasa lebih selamat dan selesa?\\n9. Adakah anda mahukan hiburan dalam pesawat?\\n10. Adakah anda memilih filem, muzik atau permainan?\\n\\nBahagian 3: Identiti Syarikat Penerbangan dan Reputasinya\\n11. Adakah anda lebih suka syarikat penerbangan tertentu?\\n12. Adakah anda merasakan bahawa syarikat penerbangan pilihan anda boleh menjamin keselamatan dan keselesaan anda semasa penerbangan?\\n13. Seberapa kerapkah anda terbang dengan syarikat penerbangan yang sama?\\n\\nBahagian 4: Keadaan Cuaca\\n14. Bagaimanakah perasaan anda apabila anda terbang dalam keadaan cuaca buruk?\\n15. Sejauh manakah kepentingan keadaan cuaca menentukan keseronokan dan keselamatan penerbangan?\\n\\nBahagian 5: Tujuan Perjalanan\\n16. Adakah anda merasakan bahawa destinasi perjalanan yang anda tuju menjejaskan persepsi anda tentang keseronokan atau keselamatan penerbangan?\\n17. Apakah tujuan perjalanan anda menunjukkan kesan ke atas bagaimana anda merasa tentang penerbangan?\\n\\nBahagian 6: Pengalaman Terbang Sebelumnya\\n18. Seberapa kerapkah anda berasa gementar atau bimbang semasa penerbangan?\\n19. Pernahkah anda mengalami kejadian yang tidak diingini semasa penerbangan? Jika ya, apakah pengalaman anda?\\n\\nBahagian 7: Pernyataan Terbang itu Seronok\\n20. Terbang itu menyeronokkan\\n    a. Berapa banyak anda bersetuju dengan pernyataan ini dari 1-10 (1= Tidak Setuju, 10= Sangat Setuju)?\\n21. Sila berikan penjelasan mengenai sebab anda memberi skor dan perasaan atau pemikiran tambahan anda berkaitan dengan penerbangan.\\n\\nTerima kasih kerana menyertai tinjauan kami dan masa yang anda berikan untuk mengisi soalan ini.<|im_end|>\\n<|im_start|>user\\nteks: Untuk menghitung persentase air dalam massa total benda, kita pertama-tama harus mengetahui berapa berat air dalam benda tersebut. Namun, karena persyaratan tambahan bahwa kandungan air harus ditentukan menggunakan metode non-destruktif, kita tidak dapat langsung memisahkan air dari batu.\\n\\nSebagai solusinya, kita dapat menggunakan teknik non-destruktif seperti analisis spektroskopi atau resonansi magnetik nuklir (NMR) untuk mengukur jumlah air dalam batu tanpa merusak atau merubah benda tersebut.\\n\\nNamun, tanpa informasi tambahan mengenai hasil dari pengukuran ini, tidak mungkin untuk memberikan persentase air dalam massa total benda. Oleh karena itu, dibutuhkan data tambahan mengenai hasil pengukuran yang dilakukan menggunakan metode non-destruktif seperti yang dijelaskan di atas.\\n\\nDengan demikian, kita dapat menyimpulkan bahwa untuk menghitung persentase air dalam massa total benda yang diberikan dengan persyaratan tambahan menggunakan metode non-destruktif, dibutuhkan informasi tambahan mengenai hasil pengukuran yang dilakukan untuk menentukan kandungan air dalam benda tersebut.\\nterjemah ke melayu baku<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nUntuk mengira peratusan air dalam jisim keseluruhan objek, kita mesti mengetahui terlebih dahulu berapa berat air dalam benda tersebut. Namun, disebabkan keperluan tambahan bahawa kandungan air perlu ditentukan menggunakan kaedah bukan pemusnah, kita tidak boleh memisahkan air daripada batu secara langsung.\\n\\nSebagai penyelesaiannya, kita boleh menggunakan teknik bukan pemusnah seperti analisis spektroskopi atau resonans magnetik nuklear (NMR) untuk mengukur jumlah air dalam batu tanpa merosakkan atau mengubah benda tersebut.\\n\\nWalau bagaimanapun, tanpa maklumat tambahan mengenai keputusan pengukuran ini, adalah mustahil untuk memberikan peratus air dalam jisim keseluruhan objek. Oleh itu, data tambahan diperlukan tentang hasil pengukuran yang dilakukan menggunakan kaedah bukan pemusnah seperti yang dijelaskan di atas.\\n\\nDengan demikian, kita dapat menyimpulkan bahawa untuk mengira peratusan air dalam jisim keseluruhan objek yang diberikan dengan syarat tambahan menggunakan kaedah bukan pemusnah, maklumat tambahan diperlukan tentang hasil pengukuran yang dijalankan untuk menentukan kandungan air dalam benda tersebut.<|im_end|>\\n<|im_start|>user\\nteks: JSON adalah singkatan dari JavaScript Object Notation. Ini adalah sebuah format untuk pertukaran data. Untuk membuat kata sandi acak dengan 10 karakter dengan setidaknya satu huruf kapital, satu angka, dan satu karakter khusus dari simbol berikut: !, ?, +, -, =, %, dan /, Anda dapat menggunakan kode JSON seperti di bawah ini:\\n\\n[{\\n  \"panjang\": 10,\\n  \"huruf_besar\": true,\\n  \"angka\": true,\\n  \"simbol\": [\"!\", \"?\", \"+\", \"-\", \"=\", \"%\", \"/\"]\\n}]\\n\\nPenjelasan kode JSON di atas adalah sebagai berikut:\\n- \"panjang\": 10, menunjukkan bahwa kata sandi harus terdiri dari 10 karakter.\\n- \"huruf_besar\": true, menunjukkan bahwa setidaknya satu huruf kapital harus ada dalam kata sandi.\\n- \"angka\": true, menunjukkan bahwa setidaknya satu angka harus ada dalam kata sandi.\\n- \"simbol\": [\"!\", \"?\", \"+\", \"-\", \"=\", \"%\", \"/\"], menunjukkan bahwa setidaknya satu karakter khusus yang berasal dari simbol yang diberikan harus ada dalam kata sandi.\\n\\nDengan menggunakan kode JSON seperti ini, Anda dapat membuat kata sandi acak yang memenuhi persyaratan yang ditetapkan dengan mudah dan cepat.\\nterjemah ke melayu baku<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nJSON adalah singkatan bagi Notasi Objek JavaScript. Ini ialah format untuk pertukaran data. Untuk membuat kata laluan rawak dengan 10 aksara dengan sekurang-kurangnya satu huruf besar, satu nombor dan satu aksara khas daripada simbol berikut:!,?, +, -, =, %, dan /, anda boleh menggunakan kod JSON seperti di bawah:\\n\\n[{\\n  \"panjang\": 10,\\n  \"huruf_besar\": true,\\n  \"angka\": true,\\n  \"simbol\": [\"!\", \"?\", \"+\", \"-\", \"=\", \"%\", \"/\"]\\n}]\\n\\nPenjelasan kod JSON di atas adalah seperti berikut:\\n- \"panjang\": 10, menunjukkan bahawa kata laluan mesti terdiri daripada 10 aksara.\\n- \"huruf_besar\": true, menunjukkan bahawa sekurang-kurangnya satu huruf besar mesti ada dalam kata laluan.\\n- \"angka\": true, menunjukkan bahawa sekurang-kurangnya satu angka mesti ada dalam kata laluan.\\n- \"simbol\": [\"!\", \"?\", \"+\", \"-\", \"=\", \"%\", \"/\"], menunjukkan bahawa sekurang-kurangnya satu aksara khas yang berasal dari simbol yang diberikan mesti ada dalam kata laluan.\\n\\nDengan menggunakan kod JSON seperti ini, anda boleh membuat kata laluan rawak yang memenuhi keperluan yang ditetapkan dengan mudah dan pantas.<|im_end|>\\n<|im_start|>user\\n`Kode di atas adalah sebuah contoh penggunaan teknik Pemrosesan Bahasa Alami untuk mengklasifikasikan kalimat yang diberikan sebagai sarkastik atau tidak. Berikut adalah penjelasan untuk setiap tahapan dalam kode tersebut:\\n\\n1. Impor library dan muat dataset:\\n```python\\nimport pandas as pd\\nimport nltk\\nnltk.download(\\'stopwords\\')\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\n\\n#muat dataset\\ndf = pd.read_csv(\"sarcasm_dataset.csv\")\\n```\\nPada bagian pertama ini, kita mengimpor library yang akan digunakan, seperti pandas dan nltk (untuk memproses teks). Lalu, kita akan menggunakan nltk untuk mengunduh daftar stopwords (kata-kata yang umumnya tidak berkontribusi dalam klasifikasi), dan mengimpor beberapa fungsi nltk lainnya seperti word_tokenize. Setelah itu, kita memuat dataset yang akan digunakan dalam format csv.\\n\\n2. Pra-pemrosesan:\\n```python\\ndf[\\'text\\'] = df[\\'text\\'].apply(lambda x: x.lower()) #huruf kecil semua\\ndf[\\'text\\'] = df[\\'text\\'].apply(lambda x: word_tokenize(x)) #tokenisasi\\nstop = set(stopwords.words(\\'english\\'))\\ndf[\\'text\\'] = df[\\'text\\'].apply(lambda x: [word for word in x if word not in stop]) #hapus kata yang tidak diperlukan\\n```\\nDalam tahap ini, kita melakukan pra-pemrosesan data untuk mengurangi keberagaman data dan membuat data menjadi seragam sehingga mudah untuk diproses. Tahapan ini termasuk mengubah semua huruf dalam teks menjadi huruf kecil, melakukan tokenisasi (memecah teks menjadi kata-kata), dan menghapus kata yang tidak diperlukan (seperti stopwords).\\n\\n3. Ekstraksi Fitur:\\n```python\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nvectorizer = CountVectorizer(max_features=2000) #mempertahankan 2000 kata yang paling sering digunakan\\nX = vectorizer.fit_transform(df[\\'text\\'].apply(lambda x: \\' \\'.join(x))).toarray()\\ny = df[\\'sarcastic\\']\\n```\\nDalam tahap ekstraksi fitur ini, kita akan mengubah kata-kata di dalam teks menjadi representasi numerical untuk dapat diproses oleh mesin. Dalam kode di atas, kita menggunakan CountVectorizer dari library scikit-learn untuk menghitung kemunculan kata-kata dalam teks dan membuat matriks numerik dari kemunculan kata-kata tersebut. Matriks ini juga dapat dipertimbangkan sebagai representasi fitur dari setiap dokumen dalam dataset. Di sini kita memilih untuk mempertahankan 2000 kata paling sering digunakan. Kita juga menyiapkan variabel y yang berisi label dari dataset (sarkastik atau tidak).\\n\\n4. Pembagian dataset:\\n```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n```\\nDalam tahap ini, kita membagi dataset menjadi dua bagian, yaitu train set dan test set. Train set digunakan untuk melatih model, sedangkan test set digunakan untuk menguji model.\\n\\n5. Pelatihan klasifikasi:\\n```python\\nfrom sklearn.ensemble import RandomForestClassifier\\nclassifier = RandomForestClassifier(n_estimators=200, random_state=42)\\nclassifier.fit(X_train, y_train)\\n```\\nDalam tahap ini, kita melakukan pelatihan model untuk mengklasifikasikan apakah suatu kalimat sarkastik atau tidak. Kita menggunakan model Random Forest Classifier dari library scikit-learn untuk melakukan klasifikasi.\\n\\n6. Prediksi pada test set:\\n```python\\ny_pred = classifier.predict(X_test)\\n```\\nSetelah melatih model, kita memprediksi label dari test set menggunakan model yang sudah dilatih.\\n\\n7. Evaluasi klasifikasi:\\n```python\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\nprint(\"Precision:\", precision_score(y_test, y_pred))\\nprint(\"Recall:\", recall_score(y_test, y_pred))\\nprint(\"F1-Score:\", f1_score(y_test, y_pred))\\n```\\nDalam tahap terakhir ini, kita mengevaluasi performa model menggunakan beberapa metrik evaluasi, seperti akurasi, presisi, recall, dan F1-Score.\\n\\nDalam keseluruhan, kode di atas akan menghasilkan sebuah model yang dapat digunakan untuk mengklasifikasikan apakah sebuah kalimat sarkastik atau tidak. Anda dapat menggunakan data sendiri dengan format yang sesuai dan melakukan klasifikasi dengan menggunakan model yang sudah dilatih.` terjemah ke malay<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nKod di atas adalah contoh penggunaan teknik Pengekstrakan Bahasa Semulajadi untuk mengklasifikasikan ayat yang diberikan sebagai sarkastik atau tidak. Berikut adalah penjelasan untuk setiap peringkat dalam kod:\\n\\n1. Import perpustakaan dan muat dataset:\\n```python\\nimport pandas as pd\\nimport nltk\\nnltk.download(\\'stopwords\\')\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\n\\n#muat dataset\\ndf = pd.read_csv(\"sarcasm_dataset.csv\")\\n```\\nPada bahagian pertama ini, kita mengimport perpustakaan yang akan digunakan, seperti pandas dan nltk (untuk memproses teks). Kemudian, kita menggunakan nltk untuk memuat turun senarai stopwords (kata-kata yang biasanya tidak menyumbang kepada pengelasan), dan mengimport beberapa fungsi nltk lain seperti word_tokenize. Selepas itu, kita memuatkan dataset yang akan digunakan dalam format csv.\\n\\n2. Pra-pemprosesan:\\n```python\\ndf[\\'text\\'] = df[\\'text\\'].apply(lambda x: x.lower()) #huruf kecil semua\\ndf[\\'text\\'] = df[\\'text\\'].apply(lambda x: word_tokenize(x)) #pengecaman\\nstop = set(stopwords.words(\\'english\\'))\\ndf[\\'text\\'] = df[\\'text\\'].apply(lambda x: [word for word in x if word not in stop]) #menghapus perkataan yang tidak diperlukan\\n```\\nPada tahap ini, kita melakukan pra-pemprosesan data untuk mengurangkan kepelbagaian data dan menjadikan data seragam supaya mudah diproses. Peringkat ini termasuk mengubah semua huruf dalam teks menjadi huruf kecil, melakukan tokenisasi (memecahkan teks menjadi kata-kata), dan menghapus perkataan yang tidak perlu (seperti stopwords).\\n\\n3. Ekstrak Ciri:\\n```python\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nvectorizer = CountVectorizer(max_features=2000) #menahan 2000 perkataan yang paling kerap digunakan\\nX = vectorizer.fit_transform(df[\\'text\\'].apply(lambda x: \\'\\'.join(x))).toarray()\\ny = df[\\'sarcastic\\']\\n```\\nDalam peringkat ekstraksi ciri ini, kita akan menukar perkataan dalam teks menjadi perwakilan berangka untuk boleh diproses oleh mesin. Dalam kod di atas, kita menggunakan CountVectorizer dari scikit-learn library untuk mengira kemunculan perkataan dalam teks dan membuat matriks berangka daripada kemunculan perkataan tersebut. Matriks ini juga boleh dipertimbangkan sebagai representasi fitur bagi setiap dokumen dalam dataset. Di sini kita memilih untuk mengekalkan 2000 perkataan yang paling sering digunakan. Kita juga menyediakan pembolehubah y yang mengandungi label dataset (sarkastik atau tidak).\\n\\n4. Pembahagian dataset:\\n```python\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n```\\nPada peringkat ini, kita membahagikan dataset ke dua bahagian, iaitu set latihan dan set ujian. Set latihan digunakan untuk melatih model, manakala set ujian digunakan untuk menguji model.\\n\\n5. Latihan klasifikasi:\\n```python\\nfrom sklearn.ensemble import RandomForestClassifier\\nclassifier = RandomForestClassifier(n_estimators=200, random_state=42)\\nclassifier.fit(X_train, y_train)\\n```\\nDalam peringkat ini, kita melakukan latihan model untuk mengklasifikasikan sama ada sesuatu ayat itu sarkastik atau tidak. Kita menggunakan model Random Forest Classifier dari scikit-learn library untuk melakukan klasifikasi.\\n\\n6. Ramalan pada set ujian:\\n```python\\ny_pred = classifier.predict(X_test)\\n```\\nSelepas melatih model, kita meramalkan label dari set ujian menggunakan model yang telah dilatih.\\n\\n7. Penilaian prestasi model:\\n```python\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\nprint(\"Precision:\", precision_score(y_test, y_pred))\\nprint(\"Recall:\", recall_score(y_test, y_pred))\\nprint(\"F1-Score:\", f1_score(y_test, y_pred))\\n```\\nDalam peringkat terakhir ini, kita menilai prestasi model menggunakan beberapa metrik penilaian, seperti ketepatan, ketepatan, penarikan balik, dan F1-Score.\\n\\nSecara keseluruhan, kod di atas akan menghasilkan model yang boleh digunakan untuk mengklasifikasikan sama ada sebuah ayat itu sarkastik atau tidak. Anda boleh menggunakan data sendiri dengan format yang sesuai dan melakukan klasifikasi dengan menggunakan model yang telah dilatih.<|im_end|>\\n<|im_start|>user\\n`Sebagai AI, saya dapat membantu dengan proses perancangan algoritma yang spesifik yang menghasilkan rencana belajar yang unik dan personal berdasarkan gaya belajar, jadwal, dan tujuan pengguna. Namun, sebagai AI, saya perlu diprogram dan diarahkan oleh engineer atau programmer yang ahli dalam bidang ini.\\n\\nProses perancangan algoritma yang efektif memerlukan pemahaman yang mendalam tentang gaya belajar seseorang. Ada beberapa jenis gaya belajar, termasuk visual, auditori, kinestetik, reflektif, dan interaktif. Algoritma perlu mempertimbangkan preferensi belajar setiap individu dan dari situ menghasilkan rekomendasi berbeda.\\n\\nSelain mempertimbangkan gaya belajar, algoritma juga perlu menyesuaikan jadwal dan preferensi waktu masing-masing pengguna. Ini termasuk faktor-faktor seperti waktu luang, waktu kerja, atau waktu di tempat lain.\\n\\nTerakhir, algoritma perlu mempertimbangkan tujuan belajar pengguna. Tujuan mungkin berbeda-beda, seperti memperbaiki kemampuan bahasa Inggris, meningkatkan kemampuan programing, atau meningkatkan keterampilan berbicara di depan umum. Algoritma harus dilengkapi dengan filter yang dapat membuat rekomendasi yang jelas dan spesifik berdasarkan tujuan akhir pengguna.\\n\\nSecara keseluruhan, perancangan algoritma yang hebat membutuhkan pemahaman mendalam tentang preferensi dan tujuan setiap pengguna. Dengan algoritma yang tepat, saya yakin kita bisa membantu pengguna untuk belajar dengan lebih efektif dan efisien.` terjemah ke malay<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nSebagai AI, saya boleh membantu dengan proses perancangan algoritma khusus yang menghasilkan rancangan pembelajaran unik dan peribadi berdasarkan gaya pembelajaran, jadual dan matlamat pengguna. Walau bagaimanapun, sebagai AI, saya perlu diprogramkan dan diarahkan oleh jurutera atau pengaturcara yang pakar dalam bidang ini.\\n\\nProses merancang algoritma yang berkesan memerlukan pemahaman mendalam tentang gaya pembelajaran seseorang. Terdapat beberapa jenis gaya pembelajaran, termasuk visual, auditori, kinestetik, reflektif dan interaktif. Algoritma perlu mempertimbangkan keutamaan belajar setiap individu dan dari situ menghasilkan cadangan yang berbeza.\\n\\nSelain mempertimbangkan gaya pembelajaran, algoritma juga perlu menyesuaikan jadual dan masa kegemaran setiap pengguna. Ini termasuk faktor seperti masa lapang, waktu kerja atau masa di tempat lain.\\n\\nAkhir sekali, algoritma perlu mempertimbangkan tujuan pembelajaran pengguna. Tujuan mungkin berbeza-beza, seperti meningkatkan kemahiran bahasa Inggeris, meningkatkan keupayaan pengaturcaraan, atau meningkatkan kemahiran bercakap di khalayak ramai. Algoritma mesti dilengkapi dengan penapis yang boleh membuat cadangan yang jelas dan spesifik berdasarkan tujuan akhir pengguna.\\n\\nSecara keseluruhan, reka bentuk algoritma yang hebat memerlukan pemahaman mendalam tentang keutamaan dan matlamat setiap pengguna. Dengan algoritma yang tepat, saya yakin kita boleh membantu pengguna untuk belajar dengan lebih efektif dan cekap.<|im_end|>\\n<|im_start|>user\\n`Dalam menambahkan persyaratan bahwa setiap karakter harus memiliki keterbatasan fisik yang memengaruhi kemampuannya untuk berkontribusi terhadap kesuksesan atau kegagalan kelompok dalam suatu cerita, Anda perlu mempertimbangkan beberapa hal yang dapat membantu menjadikan karakter tersebut lebih dihargai dan realistis.\\n\\nPertimbangan pertama adalah memikirkan keterbatasan apa yang dapat membuat karakter terlihat lebih manusiawi dan juga membantu untuk membuat cerita lebih menarik dan dramatis. Keterbatasan ini haruslah berkaitan dengan kemampuan fisik dari karakter tersebut dan haruslah diperkaya dengan detail yang cukup. Keterbatasan tersebut dapat berupa kecacatan fisik, gangguan kesehatan, atau keterampilan fisik yang terbatas.\\n\\nContohnya, karakter yang memiliki gangguan penglihatan seperti buta dapat memengaruhi kemampuannya untuk berkontribusi dalam kelompok. Namun, keterbatasan tersebut dapat diintegrasikan dengan kreatif ke dalam perspektif karakter, seperti melalui teknik penyuntingan audio atau penggunaan perangkat bantu seperti kacamata atau stik untuk membantu navigasi.\\n\\nSelanjutnya, Anda perlu memilih keterbatasan yang memengaruhi kehidupan karakter secara signifikan, tetapi juga dapat menjadi keuntungan dalam cerita. Misalnya, karakter yang memiliki keterampilan fisik yang terbatas dalam hal kecepatan dapat menjadi keunggulan saat beroperasi dalam situasi yang membutuhkan keterampilan yang lebih lambat dan perhitungan yang lebih teliti.\\n\\nMenambahkan keterbatasan fisik ke dalam karakter juga akan membantu untuk mengeksplorasi karakter secara lebih mendalam, memberikan detail tambahan tentang latar belakang karakter dan pengalaman hidup yang memengaruhi karakter tersebut. Ini akan membantu pembaca atau penonton lebih mengetahui siapa karakter tersebut dan memberikan kesempatan untuk menerapkan empati dan simpati terhadap karakter tersebut.\\n\\nTerakhir, keterbatasan fisik pada karakter haruslah diintegrasikan dengan baik ke dalam cerita dan harus diperlihatkan bagaimana karakter tersebut dapat mengatasi kekurangan tersebut untuk membuktikan dirinya sebagai anggota yang berharga dalam kelompok. Dengan begitu, akan memberikan pesan positif bagi pembaca atau penonton tentang kekuatan yang dimiliki seseorang untuk mengatasi keterbatasan fisik mereka demi mencapai keberhasilan.\\n\\nDalam kesimpulannya, menambahkan persyaratan bahwa setiap karakter harus memiliki keterbatasan fisik yang memengaruhi kemampuannya untuk berkontribusi terhadap kesuksesan atau kegagalan kelompok dalam sebuah cerita memerlukan pemikiran dan kreativitas untuk membentuk karakter yang realistis, kaya akan detail, dan memberikan pesan yang positif tentang kekuatan manusia dalam mengatasi keterbatasan mereka.` terjemah ke malay<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\nDalam menambah keperluan bahawa setiap watak mesti mempunyai batasan fizikal yang mempengaruhi keupayaannya untuk menyumbang kepada kejayaan atau kegagalan kumpulan dalam sesebuah cerita, anda perlu mempertimbangkan beberapa perkara yang boleh membantu menjadikan watak itu lebih dihargai dan realistik.\\n\\nPertimbangan pertama adalah memikirkan apakah batasan-batasan yang dapat membuat watak kelihatan lebih manusiawi dan juga membantu untuk menjadikan cerita lebih menarik dan dramatik. Sempadan ini mestilah berkaitan dengan kebolehan fizikal watak tersebut dan hendaklah diperkaya dengan perincian yang cukup. Sempadan ini boleh berupa kecacatan fizikal, gangguan kesihatan, atau kemahiran fizikal yang terhad.\\n\\nSebagai contoh, watak yang mengalami masalah penglihatan seperti buta boleh menjejaskan kemampuannya untuk menyumbang kepada kumpulan. Walau bagaimanapun, sekatan-sekatan ini boleh diintegrasikan secara kreatif ke dalam perspektif watak, seperti melalui teknik penyuntingan audio atau penggunaan alat bantuan seperti cermin mata atau kayu untuk membantu navigasi.\\n\\nSeterusnya, anda perlu memilih had yang mempengaruhi kehidupan watak secara signifikan, tetapi juga boleh menjadi kelebihan dalam cerita. Sebagai contoh, watak yang mempunyai kemahiran fizikal yang terhad dari segi kelajuan boleh menjadi kelebihan apabila beroperasi dalam situasi yang memerlukan kemahiran yang lebih perlahan dan pengiraan yang lebih teliti.\\n\\nMenambah had fizikal pada watak juga akan membantu meneroka watak dengan lebih mendalam, memberikan butiran tambahan tentang latar belakang watak dan pengalaman hidup yang mempengaruhi watak tersebut. Ini akan membantu pembaca atau penonton mengetahui lebih lanjut siapa watak tersebut dan memberi peluang untuk menerapkan empati dan simpati terhadap watak tersebut.\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[-3]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393bbf66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
